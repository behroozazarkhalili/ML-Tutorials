{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 21:21:19.133817: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-01 21:21:19.213037: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-01 21:21:20.750512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TransformerEncoderLayer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, dff: int, rate: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes the TransformerEncoderLayer.\n",
    "\n",
    "        Parameters:\n",
    "            d_model (int): The dimension of the model.\n",
    "            num_heads (int): The number of attention heads.\n",
    "            dff (int): The number of units in the feedforward neural network layer.\n",
    "            rate (float, optional): The dropout rate. Default is 0.1.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=d_model, dropout=rate\n",
    "        )\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, training: bool) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Call function for the layer.\n",
    "\n",
    "        Parameters:\n",
    "            inputs (tf.Tensor): The input tensor.\n",
    "            training (bool): Whether the model is in training mode.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        attn_output = self.multi_head_attention(inputs, inputs, return_attention_scores=False)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating PositionalEncoding Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Encoding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, position: int, d_model: int):\n",
    "        \"\"\"\n",
    "        Initialize the PositionalEncoding object.\n",
    "\n",
    "        Parameters:\n",
    "            position (int): The position parameter.\n",
    "            d_model (int): The d_model parameter.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position: tf.Tensor, i: tf.Tensor, d_model: int) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Get angles for positional encoding\n",
    "\n",
    "        Parameters:\n",
    "            position (tf.Tensor): The position tensor.\n",
    "            i (tf.Tensor): The i tensor.\n",
    "            d_model (int): The d_model parameter.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The angles tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position: int, d_model: int) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the positional encoding.\n",
    "\n",
    "        Parameters:\n",
    "            position (int): The position parameter.\n",
    "            d_model (int): The d_model parameter.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The positional encoding tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        angle_rads = self.get_angles(position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "                                     i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "                                     d_model=d_model\n",
    "                                     )\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Call function for the layer\n",
    "        \"\"\"\n",
    "        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TransformerEncoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Class.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_layers: int, \n",
    "                 d_model: int, \n",
    "                 num_heads: int, \n",
    "                 dff: int,\n",
    "                 input_vocab_size: int, \n",
    "                 rate: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes the TransformerEncoder.\n",
    "\n",
    "        Parameters:\n",
    "            num_layers (int): The number of layers.\n",
    "            d_model (int): The dimensionality of the model.\n",
    "            num_heads (int): The number of attention heads.\n",
    "            dff (int): The number of neurons in the feedforward network.\n",
    "            input_vocab_size (int): The size of the input vocabulary.\n",
    "            rate (float, optional): The dropout rate. Defaults to 0.1.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(input_vocab_size, d_model)\n",
    "\n",
    "        self.enc_layers = [TransformerEncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, x: tf.Tensor, training: bool) -> tf.Tensor:\n",
    "        \"\"\" \n",
    "        Call function for the layer.\n",
    "\n",
    "        Parameters:\n",
    "            x (tf.Tensor): The input tensor.\n",
    "            training (bool): Whether the model is in training mode.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(x.shape[-1], dtype=tf.float32))\n",
    "        x += self.pos_encoding(x)\n",
    "        for layer in self.enc_layers:\n",
    "            x = layer(x, training)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TransformerEncoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    Custom Learning Rate Schedule Class\n",
    "\n",
    "    Parameters:\n",
    "        d_model (int): The dimension of the model.\n",
    "        warmup_steps (int, optional): The number of warmup steps. Defaults to 4000.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, warmup_steps: int = 4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step: int) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Call function for the learning rate schedule.\n",
    "\n",
    "        Parameters:\n",
    "            step (int): The current step.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The learning rate tensor.\n",
    "        \"\"\"\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TransformerClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Transformer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_layers: int, \n",
    "                 d_model: int, \n",
    "                 num_heads: int, \n",
    "                 dff: int, \n",
    "                 input_vocab_size: int, \n",
    "                 rate: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model with the specified parameters.\n",
    "\n",
    "        Parameters:\n",
    "            num_layers (int): The number of layers in the Transformer model.\n",
    "            d_model (int): The dimensionality of the model.\n",
    "            num_heads (int): The number of attention heads.\n",
    "            dff (int): The dimensionality of the feed-forward layer.\n",
    "            input_vocab_size (int): The size of the input vocabulary.\n",
    "            rate (float, optional): The dropout rate. Defaults to 0.1.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_layers,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dff=dff,\n",
    "            input_vocab_size=input_vocab_size,\n",
    "            rate=rate\n",
    "            )\n",
    "\n",
    "        self.final_layer = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(input_vocab_size, activation='softmax'))\n",
    "\n",
    "    def call(self, inp: tf.Tensor, training: bool) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Call function for the Transformer model.\n",
    "\n",
    "        Parameters:\n",
    "            inp (tf.Tensor): The input tensor.\n",
    "            training (bool): The training flag.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output tensor.\n",
    "        \"\"\"\n",
    "\n",
    "        enc_output = self.encoder(inp, training)\n",
    "        return self.final_layer(enc_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_samples: int, sequence_length: int, vocab_size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to generate random data\n",
    "\n",
    "    Parameters:\n",
    "        num_samples (int): The number of samples to generate.\n",
    "        sequence_length (int): The length of the sequence.\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The generated data.\n",
    "    \"\"\"\n",
    "    return np.random.randint(0, vocab_size, size=(num_samples, sequence_length))\n",
    "\n",
    "\n",
    "def reverse_sequence(sequence: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to reverse a sequence\n",
    "\n",
    "    Parameters:\n",
    "        sequence (np.ndarray): The sequence to reverse.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The reversed sequence.\n",
    "    \"\"\"\n",
    "    return sequence[:, ::-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "num_layers_enc = 4\n",
    "d_model_enc = 128\n",
    "num_heads_enc = 8\n",
    "dff_enc = 512\n",
    "input_vocab_size_model = 20\n",
    "batch_size = 64\n",
    "sequence_length = 15\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random training data\n",
    "x_train = generate_data(1000, sequence_length, input_vocab_size_model)\n",
    "y_train = reverse_sequence(x_train)[..., np.newaxis]\n",
    "\n",
    "# Generate random test data\n",
    "x_test = generate_data(1000, sequence_length, input_vocab_size_model)\n",
    "y_test = reverse_sequence(x_test)[..., np.newaxis]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-01 21:21:23.647628: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 16s 301ms/step - loss: 3.9893 - accuracy: 0.0457\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 5s 302ms/step - loss: 3.7169 - accuracy: 0.0468\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 5s 318ms/step - loss: 3.4233 - accuracy: 0.0504\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 7s 459ms/step - loss: 3.2316 - accuracy: 0.0538\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 8s 467ms/step - loss: 3.1167 - accuracy: 0.0581\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 8s 507ms/step - loss: 3.0589 - accuracy: 0.0644\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 12s 748ms/step - loss: 3.0129 - accuracy: 0.0766\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 11s 700ms/step - loss: 2.9899 - accuracy: 0.0877\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 11s 709ms/step - loss: 2.9626 - accuracy: 0.0975\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 11s 662ms/step - loss: 2.9377 - accuracy: 0.1067\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 9s 590ms/step - loss: 2.9061 - accuracy: 0.1221\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 9s 583ms/step - loss: 2.8575 - accuracy: 0.1362\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 8s 460ms/step - loss: 2.7979 - accuracy: 0.1512\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 7s 442ms/step - loss: 2.7066 - accuracy: 0.1748\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 7s 449ms/step - loss: 2.5958 - accuracy: 0.1927\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 7s 444ms/step - loss: 2.4443 - accuracy: 0.2279\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 7s 445ms/step - loss: 2.2331 - accuracy: 0.2753\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 7s 420ms/step - loss: 1.9396 - accuracy: 0.3441\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 4s 280ms/step - loss: 1.4861 - accuracy: 0.4714\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 4s 280ms/step - loss: 0.9169 - accuracy: 0.6769\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 5s 285ms/step - loss: 0.4715 - accuracy: 0.8415\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 5s 285ms/step - loss: 0.2166 - accuracy: 0.9362\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 5s 284ms/step - loss: 0.1118 - accuracy: 0.9749\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 5s 286ms/step - loss: 0.0689 - accuracy: 0.9867\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 5s 301ms/step - loss: 0.0493 - accuracy: 0.9910\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 5s 291ms/step - loss: 0.0389 - accuracy: 0.9917\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 5s 286ms/step - loss: 0.0295 - accuracy: 0.9952\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 5s 293ms/step - loss: 0.0275 - accuracy: 0.9955\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 5s 284ms/step - loss: 0.0254 - accuracy: 0.9955\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 5s 285ms/step - loss: 0.0251 - accuracy: 0.9951\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 5s 298ms/step - loss: 0.0324 - accuracy: 0.9914\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 5s 293ms/step - loss: 0.0222 - accuracy: 0.9951\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 5s 291ms/step - loss: 0.0149 - accuracy: 0.9979\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 5s 295ms/step - loss: 0.0126 - accuracy: 0.9983\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 5s 283ms/step - loss: 0.0134 - accuracy: 0.9974\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 5s 290ms/step - loss: 0.0153 - accuracy: 0.9965\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 5s 291ms/step - loss: 0.0145 - accuracy: 0.9967\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 5s 297ms/step - loss: 0.0413 - accuracy: 0.9875\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 5s 294ms/step - loss: 0.1323 - accuracy: 0.9606\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 5s 289ms/step - loss: 0.0942 - accuracy: 0.9739\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 5s 290ms/step - loss: 0.0263 - accuracy: 0.9944\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 5s 291ms/step - loss: 0.0127 - accuracy: 0.9979\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 5s 294ms/step - loss: 0.0068 - accuracy: 0.9988\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 5s 313ms/step - loss: 0.0060 - accuracy: 0.9992\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 5s 292ms/step - loss: 0.0048 - accuracy: 0.9995\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 5s 294ms/step - loss: 0.0041 - accuracy: 0.9996\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 5s 302ms/step - loss: 0.0042 - accuracy: 0.9993\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 5s 307ms/step - loss: 0.0036 - accuracy: 0.9995\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 5s 303ms/step - loss: 0.0049 - accuracy: 0.9991\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 5s 316ms/step - loss: 0.0040 - accuracy: 0.9996\n",
      "32/32 [==============================] - 3s 65ms/step - loss: 1.7523e-04 - accuracy: 1.0000\n",
      "Evaluation Loss: 0.00017523116548545659\n",
      "Evaluation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create Transformer model\n",
    "transformer_model = Transformer(\n",
    "    num_layers=num_layers_enc,\n",
    "    d_model=d_model_enc,\n",
    "    num_heads=num_heads_enc,\n",
    "    dff=dff_enc,\n",
    "    input_vocab_size=input_vocab_size_model\n",
    "    )\n",
    "\n",
    "# Define the optimizer with a custom learning rate schedule\n",
    "custom_learning_rate = CustomSchedule(d_model_enc)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=custom_learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "transformer_model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Train the model\n",
    "transformer_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluation = transformer_model.evaluate(x_test, y_test)\n",
    "print(\"Evaluation Loss:\", evaluation[0])\n",
    "print(\"Evaluation Accuracy:\", evaluation[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
